{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO+TqHUir39q2XHvKeeZOkL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AravindBiswas/MyStudy/blob/master/AdaBoostClassifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n"
      ],
      "metadata": {
        "id": "B7NEs_Eg2yec"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Read the data file, assuming space separation.\n",
        "# We will drop the first column (index 0) which likely contains non-numerical data.\n",
        "df = pd.read_csv('/content/letterCG.data', sep=' ', header=None)\n",
        "print(f\"Shape after reading CSV: {df.shape}\") # Check shape after reading\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B2TRZ8vA3Clk",
        "outputId": "4550aa54-2878-43d8-9ce6-3c97838cb98d"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape after reading CSV: (1510, 19)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop the first column. The remaining columns are assumed to be features.\n",
        "# We also need to explicitly convert the remaining columns to a numeric type,\n",
        "# handling potential errors by coercing invalid parsing into NaN and then dropping NaNs.\n",
        "df = df.drop(df.columns[0], axis=1)\n",
        "print(f\"Shape after dropping first column: {df.shape}\") # Check shape after dropping\n",
        "\n",
        "df = df.apply(pd.to_numeric, errors='coerce')\n",
        "print(f\"Shape after converting to numeric: {df.shape}\") # Check shape after converting\n",
        "\n",
        "df = df.dropna() # Drop rows that resulted in NaN after coercion\n",
        "print(f\"Shape after dropping NaNs: {df.shape}\") # Check shape after dropping NaNs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9DSNPtlm3C0i",
        "outputId": "75de0e74-d365-40df-bc11-04146d478f09"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape after dropping first column: (1510, 18)\n",
            "Shape after converting to numeric: (1510, 18)\n",
            "Shape after dropping NaNs: (0, 18)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now, the entire DataFrame 'df' contains only numerical features.\n",
        "# We need to redefine X and y based on this numerical DataFrame.\n",
        "# It seems the user intends to use all remaining columns as features,\n",
        "# but they haven't specified a target variable in this scenario.\n",
        "# Assuming for the purpose of making the code run that the LAST column\n",
        "# of the *remaining* data is the target, and the rest are features.\n",
        "# This is a different interpretation than the previous attempt,\n",
        "# but aligns with having only numerical data.\n",
        "\n",
        "# Set all columns except the last as features and the last column as the target variable\n",
        "# Only proceed if df is not empty\n",
        "if not df.empty:\n",
        "    X = df.iloc[:, :-1]\n",
        "    y = df.iloc[:, -1]"
      ],
      "metadata": {
        "id": "hbB_o-Wo3C3l"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Read the data file, assuming space separation.\n",
        "# Based on typical structure of letterCG.data, the first column is the letter (target).\n",
        "# The remaining columns are assumed to be numerical features.\n",
        "# We will read all columns initially.\n",
        "df = pd.read_csv('/content/letterCG.data', sep=' ', header=None)\n",
        "print(f\"Shape after reading CSV: {df.shape}\") # Check shape after reading\n",
        "\n",
        "# Assuming the first column (index 0) is the target variable 'y'\n",
        "# and the rest (index 1 onwards) are features 'X'.\n",
        "# The first column contains letters which need to be encoded for classification.\n",
        "# The remaining columns (1 to 16 based on dataset description) should be numeric features.\n",
        "\n",
        "# Separate features (X) and target (y)\n",
        "# X will be all columns except the first one (index 1 onwards)\n",
        "# y will be the first column (index 0)\n",
        "X = df.iloc[:, 1:]\n",
        "y = df.iloc[:, 0]\n",
        "\n",
        "# Convert the feature columns to numeric, coercing errors to NaN and then dropping rows with NaNs.\n",
        "# This ensures X contains only numerical data suitable for the model.\n",
        "X = X.apply(pd.to_numeric, errors='coerce')\n",
        "print(f\"Shape of X after converting to numeric: {X.shape}\") # Check shape after converting\n",
        "\n",
        "# Now, drop rows where *any* feature value is NaN.\n",
        "# We must also drop the corresponding rows in 'y' to keep X and y aligned.\n",
        "rows_with_nan = X.isnull().any(axis=1)\n",
        "X = X[~rows_with_nan]\n",
        "y = y[~rows_with_nan] # Keep the corresponding rows in y\n",
        "\n",
        "print(f\"Shape of X after dropping rows with NaNs: {X.shape}\") # Check shape after dropping NaNs\n",
        "print(f\"Shape of y after dropping rows with NaNs: {y.shape}\") # Check shape after dropping NaNs\n",
        "\n",
        "# Encode the target variable 'y' (letters) into numerical labels\n",
        "# AdaBoostClassifier requires integer labels for classification.\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "print(f\"Unique classes in original y: {y.unique()}\") # Print original classes\n",
        "print(f\"Unique classes in encoded y: {np.unique(y_encoded)}\") # Print encoded classes\n",
        "\n",
        "\n",
        "# Only proceed if X is not empty and y is not empty\n",
        "if not X.empty and len(y_encoded) == len(X):\n",
        "\n",
        "    # Split the data into train and test sets using the encoded target variable\n",
        "    X_train, X_test, y_train_encoded, y_test_encoded = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Fit a sequence of AdaBoostClassifier with varying numbers of weak learners\n",
        "    train_accuracies = []\n",
        "    test_accuracies = []\n",
        "    weak_learners = range(1, 17)\n",
        "\n",
        "    # With max_depth as 1\n",
        "    for n_estimators in weak_learners:\n",
        "        # Initialize the AdaBoostClassifier with a DecisionTreeClassifier base estimator\n",
        "        # DecisionTreeClassifier with max_depth=1 is a common \"weak learner\" for AdaBoost.\n",
        "        model = AdaBoostClassifier(estimator=DecisionTreeClassifier(max_depth=1),\n",
        "                                   n_estimators=n_estimators,\n",
        "                                   random_state=42)\n",
        "\n",
        "        # Fit the model to the training data using the encoded target variable.\n",
        "        model.fit(X_train, y_train_encoded)\n",
        "\n",
        "        # Make predictions on the training and test sets\n",
        "        y_pred_train_encoded = model.predict(X_train)\n",
        "        y_pred_test_encoded = model.predict(X_test)\n",
        "\n",
        "        # Calculate the accuracy for both training and test sets\n",
        "        # Use the encoded true labels and predicted labels for accuracy calculation.\n",
        "        train_accuracy = accuracy_score(y_train_encoded, y_pred_train_encoded)\n",
        "        test_accuracy = accuracy_score(y_test_encoded, y_pred_test_encoded)\n",
        "\n",
        "        # Append the calculated accuracies to the respective lists\n",
        "        train_accuracies.append(train_accuracy)\n",
        "        test_accuracies.append(test_accuracy)\n",
        "\n",
        "    # Plot the training and test accuracies\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(weak_learners, train_accuracies, label='Training Accuracy')\n",
        "    plt.plot(weak_learners, test_accuracies, label='Test Accuracy')\n",
        "    plt.xlabel('Number of Weak Learners')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title('AdaBoostClassifier with max_depth=1')\n",
        "    plt.legend() # Add legend to differentiate lines\n",
        "    plt.show() # Show the plot\n",
        "\n",
        "else:\n",
        "    print(\"DataFrame X is empty or y does not match X size after cleaning. Cannot proceed with training.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MZ5vSuxJ3R93",
        "outputId": "429f93f0-39a1-4990-ffb6-27d1ea54baf4"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape after reading CSV: (1510, 19)\n",
            "Shape of X after converting to numeric: (1510, 18)\n",
            "Shape of X after dropping rows with NaNs: (0, 18)\n",
            "Shape of y after dropping rows with NaNs: (0,)\n",
            "Unique classes in original y: []\n",
            "Unique classes in encoded y: []\n",
            "DataFrame X is empty or y does not match X size after cleaning. Cannot proceed with training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# With max_depth as 2\n",
        "# Ensure this block is also within the data validation check\n",
        "if not X.empty and len(y_encoded) == len(X):\n",
        "    train_accuracies = []\n",
        "    test_accuracies = []\n",
        "    # Use the same weak_learners range as before\n",
        "    # weak_learners = range(1, 17) # Assumed to be defined earlier\n",
        "\n",
        "    for n_estimators in weak_learners:\n",
        "        # Initialize the AdaBoostClassifier with a DecisionTreeClassifier base estimator\n",
        "        model = AdaBoostClassifier(estimator=DecisionTreeClassifier(max_depth=2),\n",
        "                                   n_estimators=n_estimators,\n",
        "                                   random_state=42)\n",
        "\n",
        "        # Fit the model to the training data using the encoded target variable.\n",
        "        model.fit(X_train, y_train_encoded) # Use y_train_encoded\n",
        "\n",
        "        # Make predictions on the training and test sets\n",
        "        y_pred_train_encoded = model.predict(X_train) # Predict returns encoded labels\n",
        "        y_pred_test_encoded = model.predict(X_test)   # Predict returns encoded labels\n",
        "\n",
        "        # Calculate the accuracy for both training and test sets\n",
        "        # Use the encoded true labels and predicted labels for accuracy calculation.\n",
        "        train_accuracy = accuracy_score(y_train_encoded, y_pred_train_encoded) # Use y_train_encoded\n",
        "        test_accuracy = accuracy_score(y_test_encoded, y_pred_test_encoded)   # Use y_test_encoded\n",
        "\n",
        "        train_accuracies.append(train_accuracy)\n",
        "        test_accuracies.append(test_accuracy)\n",
        "\n",
        "    # Plot the training and test accuracies\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(weak_learners, train_accuracies, label='Training Accuracy')\n",
        "    plt.plot(weak_learners, test_accuracies, label='Test Accuracy')\n",
        "    plt.xlabel('Number of Weak Learners')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title('AdaBoostClassifier with max_depth=2')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"DataFrame X is empty or y does not match X size after cleaning. Cannot proceed with training for max_depth=2.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2pd9l9qI3C9T",
        "outputId": "1cf30972-7157-4bf4-9d7d-58d746432961"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrame X is empty or y does not match X size after cleaning. Cannot proceed with training for max_depth=2.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from scipy import stats # Import scipy.stats for statistical tests\n",
        "\n",
        "# Read the data file, assuming space separation.\n",
        "# Based on typical structure of letterCG.data, the first column is the letter (target).\n",
        "# The remaining columns are assumed to be numerical features.\n",
        "# We will read all columns initially.\n",
        "df = pd.read_csv('/content/letterCG.data', sep=' ', header=None)\n",
        "print(f\"Shape after reading CSV: {df.shape}\") # Check shape after reading\n",
        "\n",
        "# Assuming the first column (index 0) is the target variable 'y'\n",
        "# and the rest (index 1 onwards) are features 'X'.\n",
        "# The first column contains letters which need to be encoded for classification.\n",
        "# The remaining columns (1 to 16 based on dataset description) should be numeric features.\n",
        "\n",
        "# Separate features (X) and target (y)\n",
        "# X will be all columns except the first one (index 1 onwards)\n",
        "# y will be the first column (index 0)\n",
        "X = df.iloc[:, 1:]\n",
        "y = df.iloc[:, 0]\n",
        "\n",
        "# Convert the feature columns to numeric, coercing errors to NaN and then dropping rows with NaNs.\n",
        "# This ensures X contains only numerical data suitable for the model.\n",
        "X = X.apply(pd.to_numeric, errors='coerce')\n",
        "print(f\"Shape of X after converting to numeric: {X.shape}\") # Check shape after converting\n",
        "\n",
        "# Now, drop rows where *any* feature value is NaN.\n",
        "# We must also drop the corresponding rows in 'y' to keep X and y aligned.\n",
        "rows_with_nan = X.isnull().any(axis=1)\n",
        "X = X[~rows_with_nan]\n",
        "y = y[~rows_with_nan] # Keep the corresponding rows in y\n",
        "\n",
        "print(f\"Shape of X after dropping rows with NaNs: {X.shape}\") # Check shape after dropping NaNs\n",
        "print(f\"Shape of y after dropping rows with NaNs: {y.shape}\") # Check shape after dropping NaNs\n",
        "\n",
        "# Encode the target variable 'y' (letters) into numerical labels\n",
        "# AdaBoostClassifier requires integer labels for classification.\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "print(f\"Unique classes in original y: {y.unique()}\") # Print original classes\n",
        "print(f\"Unique classes in encoded y: {np.unique(y_encoded)}\") # Print encoded classes\n",
        "\n",
        "\n",
        "# Only proceed if X is not empty and y is not empty\n",
        "if not X.empty and len(y_encoded) == len(X):\n",
        "\n",
        "    # Split the data into train and test sets using the encoded target variable\n",
        "    X_train, X_test, y_train_encoded, y_test_encoded = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Fit a sequence of AdaBoostClassifier with varying numbers of weak learners\n",
        "    # Store test accuracies for both max_depth settings\n",
        "    test_accuracies_depth1 = []\n",
        "    test_accuracies_depth2 = []\n",
        "    weak_learners = range(1, 17)\n",
        "\n",
        "    # With max_depth as 1\n",
        "    print(\"\\nTraining with max_depth = 1...\")\n",
        "    for n_estimators in weak_learners:\n",
        "        # Initialize the AdaBoostClassifier with a DecisionTreeClassifier base estimator\n",
        "        model = AdaBoostClassifier(estimator=DecisionTreeClassifier(max_depth=1),\n",
        "                                   n_estimators=n_estimators,\n",
        "                                   random_state=42)\n",
        "\n",
        "        # Fit the model\n",
        "        model.fit(X_train, y_train_encoded)\n",
        "\n",
        "        # Make predictions and calculate accuracy on the test set\n",
        "        y_pred_test_encoded = model.predict(X_test)\n",
        "        test_accuracy = accuracy_score(y_test_encoded, y_pred_test_encoded)\n",
        "\n",
        "        # Append the test accuracy\n",
        "        test_accuracies_depth1.append(test_accuracy)\n",
        "\n",
        "    # Plot the test accuracies for max_depth=1\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(weak_learners, test_accuracies_depth1, label='Test Accuracy (max_depth=1)')\n",
        "    plt.xlabel('Number of Weak Learners')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title('AdaBoostClassifier Test Accuracy (max_depth=1)')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    # With max_depth as 2\n",
        "    print(\"\\nTraining with max_depth = 2...\")\n",
        "    for n_estimators in weak_learners:\n",
        "        # Initialize the AdaBoostClassifier with a DecisionTreeClassifier base estimator\n",
        "        model = AdaBoostClassifier(estimator=DecisionTreeClassifier(max_depth=2),\n",
        "                                   n_estimators=n_estimators,\n",
        "                                   random_state=42)\n",
        "\n",
        "        # Fit the model\n",
        "        model.fit(X_train, y_train_encoded)\n",
        "\n",
        "        # Make predictions and calculate accuracy on the test set\n",
        "        y_pred_test_encoded = model.predict(X_test)\n",
        "        test_accuracy = accuracy_score(y_test_encoded, y_pred_test_encoded)\n",
        "\n",
        "        # Append the test accuracy\n",
        "        test_accuracies_depth2.append(test_accuracy)\n",
        "\n",
        "    # Plot the test accuracies for max_depth=2\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(weak_learners, test_accuracies_depth2, label='Test Accuracy (max_depth=2)')\n",
        "    plt.xlabel('Number of Weak Learners')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title('AdaBoostClassifier Test Accuracy (max_depth=2)')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    # Combine plots for comparison\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(weak_learners, test_accuracies_depth1, label='Test Accuracy (max_depth=1)')\n",
        "    plt.plot(weak_learners, test_accuracies_depth2, label='Test Accuracy (max_depth=2)')\n",
        "    plt.xlabel('Number of Weak Learners')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title('AdaBoostClassifier Test Accuracy Comparison')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "    # Perform a paired t-test on the test accuracies\n",
        "    # This tests if the mean accuracy across the range of n_estimators is significantly different\n",
        "    # for max_depth=1 vs max_depth=2.\n",
        "    # Null Hypothesis (H0): The true mean accuracy for max_depth=1 is equal to the true mean accuracy for max_depth=2.\n",
        "    # Alternative Hypothesis (H1): The true mean accuracy for max_depth=1 is different from the true mean accuracy for max_depth=2.\n",
        "    t_statistic, p_value = stats.ttest_rel(test_accuracies_depth1, test_accuracies_depth2)\n",
        "\n",
        "    print(\"\\n--- Paired T-test Results ---\")\n",
        "    print(f\"Comparing Test Accuracies for max_depth=1 vs max_depth=2 (across n_estimators 1-16)\")\n",
        "    print(f\"T-statistic: {t_statistic:.4f}\")\n",
        "    print(f\"P-value: {p_value:.4f}\")\n",
        "\n",
        "    # Check for statistical significance\n",
        "    alpha = 0.05\n",
        "    if p_value <= alpha:\n",
        "        print(f\"Conclusion: Since the p-value ({p_value:.4f}) is <= alpha ({alpha}), we reject the null hypothesis.\")\n",
        "        print(\"There is a statistically significant difference in test accuracy between max_depth=1 and max_depth=2.\")\n",
        "    else:\n",
        "        print(f\"Conclusion: Since the p-value ({p_value:.4f}) is > alpha ({alpha}), we fail to reject the null hypothesis.\")\n",
        "        print(\"There is no statistically significant difference in test accuracy between max_depth=1 and max_depth=2.\")\n",
        "\n",
        "else:\n",
        "    print(\"DataFrame X is empty or y does not match X size after cleaning. Cannot proceed with training.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dganlPQZ3DAH",
        "outputId": "5754ac1c-206f-4ead-ceba-19c5c858b7ed"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape after reading CSV: (1510, 19)\n",
            "Shape of X after converting to numeric: (1510, 18)\n",
            "Shape of X after dropping rows with NaNs: (0, 18)\n",
            "Shape of y after dropping rows with NaNs: (0,)\n",
            "Unique classes in original y: []\n",
            "Unique classes in encoded y: []\n",
            "DataFrame X is empty or y does not match X size after cleaning. Cannot proceed with training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rIgXekpW3DCr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}