{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNWBoZMsVd4NmxHXxdg+JVp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AravindBiswas/MyStudy/blob/master/Tensorflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OZ0_-zLTq5SL",
        "outputId": "e5414142-3ce5-4bd3-ac38-d85a77fd9b7a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-04-27 06:05:34--  https://www.kaggle.com/crowdflower/twitter-airline-sentiment/home\n",
            "Resolving www.kaggle.com (www.kaggle.com)... 35.244.233.98\n",
            "Connecting to www.kaggle.com (www.kaggle.com)|35.244.233.98|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /datasets/crowdflower/twitter-airline-sentiment/home [following]\n",
            "--2025-04-27 06:05:35--  https://www.kaggle.com/datasets/crowdflower/twitter-airline-sentiment/home\n",
            "Reusing existing connection to www.kaggle.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: ‘home.1’\n",
            "\n",
            "home.1                  [ <=>                ]  10.16K  --.-KB/s    in 0.008s  \n",
            "\n",
            "2025-04-27 06:05:35 (1.22 MB/s) - ‘home.1’ saved [10408]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://www.kaggle.com/crowdflower/twitter-airline-sentiment/home"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WjZ_lyaI2CBu",
        "outputId": "8d473611-399c-43d0-fa16-5807f38edfb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!find / -name \"*.csv\"\n",
        "!ls /content/home"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Gka8Byc5BYD",
        "outputId": "651e1760-ecc5-4415-b4d4-63322478b3e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/home\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report"
      ],
      "metadata": {
        "id": "6huBhCXi09Qo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the training data\n",
        "def read_data(file_name):\n",
        "    data = pd.read_csv(file_name)\n",
        "    data = data[['airline_sentiment', 'text']]\n",
        "    return data"
      ],
      "metadata": {
        "id": "91TY3_Vm1DTl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming the file name is Tweets.csv (check the actual file name using !ls)\n",
        "file_path = '/content/Tweets.csv'  # Update the file path based on the output of !pwd and !ls\n",
        "\n",
        "# Read the CSV file\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Print the first few lines of the file\n",
        "print(data.head())"
      ],
      "metadata": {
        "id": "kucs2PHS3CDe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Observe randomly generated 10 tweets for each sentiment\n",
        "def observe_tweets(data):\n",
        "    for sentiment in data['airline_sentiment'].unique():\n",
        "        print(f\"Sentiment: {sentiment}\")\n",
        "        print(data[data['airline_sentiment'] == sentiment].sample(10))"
      ],
      "metadata": {
        "id": "cOJMpNpB1K4-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean the tweet text\n",
        "def clean_text(data):\n",
        "    data['clean_text'] = data['text'].apply(lambda x: re.sub(r'[@#]\\w*', '', x))\n",
        "    data['clean_text'] = data['clean_text'].apply(lambda x: re.sub(r'http\\S+', '', x))\n",
        "    data['clean_text'] = data['clean_text'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x))\n",
        "    return data"
      ],
      "metadata": {
        "id": "5gc_lnYd3GO2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List down the most common 15 words for each sentiment\n",
        "def common_words(data):\n",
        "    for sentiment in data['airline_sentiment'].unique():\n",
        "        print(f\"Sentiment: {sentiment}\")\n",
        "        print(data[data['airline_sentiment'] == sentiment]['clean_text'].str.split().explode().value_counts().head(15))\n"
      ],
      "metadata": {
        "id": "tfGZDHQp3Kkb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Stopwords\n",
        "def remove_stopwords(data):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    data['clean_text_no_stopwords'] = data['clean_text'].apply(lambda x: ' '.join([word for word in word_tokenize(x) if word.lower() not in stop_words]))\n",
        "    return data"
      ],
      "metadata": {
        "id": "FPb_F6yW3NYN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove specific words\n",
        "def remove_specific_words(data):\n",
        "    words_to_remove = ['americanair', 'united', 'delta', 'southwestair', 'jetblue', 'virginamerica', 'usairways', 'flight', 'plane']\n",
        "    data['clean_text_final'] = data['clean_text_no_stopwords'].apply(lambda x: ' '.join([word for word in word_tokenize(x) if word.lower() not in words_to_remove]))\n",
        "    return data"
      ],
      "metadata": {
        "id": "ctrkV3RU3P8D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode Sentiments\n",
        "def encode_sentiments(data):\n",
        "    le = LabelEncoder()\n",
        "    data['sentiment_encoded'] = le.fit_transform(data['airline_sentiment'])\n",
        "    return data"
      ],
      "metadata": {
        "id": "-DMOryAS3Uan"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorize the Text Column\n",
        "def vectorize_text(data):\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    X = vectorizer.fit_transform(data['clean_text_final'])\n",
        "    return X"
      ],
      "metadata": {
        "id": "q3iT_9wR3Xzb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare a multiclass Classification model\n",
        "def prepare_model(X, y):\n",
        "    model = MultinomialNB()\n",
        "    model.fit(X, y)\n",
        "    return model"
      ],
      "metadata": {
        "id": "3MVftLar3bGy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main function\n",
        "def main():\n",
        "    train_data = read_data('Tweets-train.csv')\n",
        "    observe_tweets(train_data)\n",
        "    train_data = clean_text(train_data)\n",
        "    common_words(train_data)\n",
        "    train_data = remove_stopwords(train_data)\n",
        "    common_words(train_data)\n",
        "    train_data = remove_specific_words(train_data)\n",
        "    common_words(train_data)\n",
        "    train_data = encode_sentiments(train_data)\n",
        "    X = vectorize_text(train_data)\n",
        "    y = train_data['sentiment_encoded']\n",
        "    model = prepare_model(X, y)\n",
        "\n",
        "    test_data = read_data('Tweets-test.csv')\n",
        "    test_data = clean_text(test_data)\n",
        "    test_data = remove_stopwords(test_data)\n",
        "    test_data = remove_specific_words(test_data)\n",
        "    test_data = encode_sentiments(test_data)\n",
        "    X_test = vectorizer.transform(test_data['clean_text_final'])\n",
        "    y_pred = model.predict(X_test)\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(confusion_matrix(test_data['sentiment_encoded'], y_pred))\n",
        "    print(\"Accuracy:\", accuracy_score(test_data['sentiment_encoded'], y_pred))\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(test_data['sentiment_encoded'], y_pred))\n",
        "\n"
      ],
      "metadata": {
        "id": "shSwZ6cxq-y_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}